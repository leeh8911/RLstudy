<요약>
1. 명시적인 정책과 내재적인 정책
	* 정책 이터레이션은 명시적인 정책이 있고, 이를 평가하는 도구로서 가치함수를 사용(정책과 가치가 명확하게 분리)
	* 정책과 가치함수가 명확히 구분되어 있어, 정책이 독립적이므로 결정적/확률적 정책 모두 가능
	* 가치 이터레이션은 정책함수가 명시적으로 있는게 아니기 때문에, 가치함수 안에 내재적으로 포함되어 있다고 생각
2. 벨만 최적 방정식과 가치 이터레이션
	* 벨만 기대 방정식을 통해 푼 문제의 해답은 `현재 정책`을 따라 갔을 때 받는 참 보상
		1. 가치함수를 현재 정책에 대한 가치함수라고 가정
		2. 반복
		3. 현재 정책에 대한 참 가치함수
	* 벨만 최적 방정식을 통해 푼 문제의 해답은 최적 가치함수
		1. 가치함수를 최적 정책에 대한 가치함수라고 가정
		2. 반복
		3. 최적 정책에 대한 참 가치함수, 최적 가치함수
	* 최적 정책을 가정하기 때문에, 정책 발전이 필요 없음
	* 벨만 최적 방정식은 기대 방정식과 다르게  max를 사용
	* 가치 이터레이션은 현재 상태에서 가능한 $R_{t+1}+{\gamma}v_{k}(S_{t+1})$의 값들 중 최고의 값으로만 업데이트 함
3. 가치 이터레이션 코드 설명
	* 정책 이터레이션과의 차이는 정책의 경우는 평가와 발전단계가 나누어져 있지만, 가치 이터레이션은 없음
	* Value_iteration함수에서는 가능한 모든 상태에 대해서 가능한 모든 행동에 대해 가치함수를 계산하지만, 다음 가치함수에 대입되는 것은 최대의 값 하나
4. 가치 이터레이션 코드 실행

<인상깊은 점>
* 깊이있는 이해 없이 유사한 용어를 반복해서 보니 헷갈리게 되는 부분이 있다(가치 함수/ 정책/ 큐 함수 등)

<질문>
* Value iteration과 policy iteration은 MDP를 풀기위한 서로 다른 방법론인가?(또한 벨만 기대 방정식과 벨만 최적 방정식도 마찬가지로 궁금)
