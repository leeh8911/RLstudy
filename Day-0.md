
<요약>
	주어진 환경의 상태(state)에 대해 내부적인 정책(policy)을 통하여 행동(action)을 결정하고 이에 대한 보상을 획득하는 에이전트가 있다면, 강화학습은 감가율(discount factor), 가치 함수(value function), 큐 함수(q function)를 통해 최적의 정책을 찾는 방법이다.
	* 가치함수 : 에이전트가 특정 상태일 때 갖는 보상을계산하는 함수
	* 큐 함수 : 에이전트가 특정 상태이고 어떤 행동을 했을 때 갖는 보상을 계산하는 함수
	* 큐함수와 가치함수의 관계는 다음과 같다. 정책에 따라 특정 확률로 어떤 행동을 할 때, 행동에 따른 확률들과 큐함수들의 곱을 모두 더한것이 가치함수
	—> $v_{\pi}(s) = {\SIGMA_{a \in A}}{\pi}(a|s)q_{\pi}(s,a)$

<인상깊은 점>
	지도/비지도 학습에 비해 강화학습에 대해서는 알고있는 바가 적어, 상대적으로 불분명한 학습방식이라고 생각하였다.  하지만, 상대적으로 단순하게 정의되는 지도/비지도 학습에 비해 강화학습은 복잡한 시스템이라고 느껴진다.

<질문>
	식 2.23에서 2.24로 넘어갈 때 반환값(G_{t+1})이 다음 시점의 가치함수(v(S_{t+1})로 바뀌는 이유? 방법?
