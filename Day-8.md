<요약>
1. 시간차 예측
	* 강화학습에서 가장 중요한 아이디어(Temporal-Difference)
	* 에피소드 마다 업데이트하는 Monte Carlo Prediction과 달리 타임스텝 마다 가치함수를 업데이트 하는 방법(Temporal-Difference Prediction)
	* MCP는 예측 결과를 가치함수에 업데이트 하는데, $G_t$는 에피소드가 끝나야 결과를 알 수 있음
		$V(S_t) <- V(S_t) + {\alpha}(G_t - V(S_t))$
	* TDP는 MCP와 달리 반환값($G_t$)를 사용하지 않고 $R_{t+1}+{\gamma}v_{\pi}(s’)$을 사용함
		$v_{\pi}(s) = E_{\pi}[R_{t+1} + {\gamma}v_{\pi}(S_{t+1})|S_t = s]$
	* 매 타임스텝마나 에이전트는 현재 상태($S_t$)에서 행동($A_t = a$) 하나 선택하고 환경으로부터 보상($R$)을 받고 다음 상태($S_{t+1}$)를 알게됨
> 		$V(S_t) <- V(S_t) + {\alpha}(R+{\gamma}V(S_{t+1}) - V(S_t))$
> 		(1) 업데이트의 목표 : R + {\gamma}V(S_{t+1})
> 		(2) 업데이트의 크기 : {\alpha}(R+{\gamma}V(S_{t+1}) - V(S_t))
	* (2)를 `시간차 에러, Temporal-Difference Error`라고 정의
	* TDP에서 업데이트의 목표는 반환값과 달리 실제값이 아니고, 다른 상태의 값을 가지고 현재 상태를 예측하는 방식으로 Bootstrap이라고 부름
	* 어떤 상태에서 행동을 하면, 보상을 받고 다음상태를 알게 되고, 다음 상태의 가치함수와 알게된 보상을 더해 그 값을 업데이트 목표로 삼음
> 		S_t     ———{A_t}————> S_{t+1}
> 		V(S_t) < —{R_{t+1}——  V_(S_{t+1})
	* TDP는 에피소드가 끝날 때까지 기다릴 필요가 없지만, 충분히 많은 샘플링을 통해 업데이트를 하면 참 가치함수에 수렴하며 많은 경우 MCP보다 효율적으로 빠른 시간에 참 가치함수에 근접함
3. 살사(S-A-R-S’-A’)
	* 실질적인 강화학습의 시작
	* 정책 이터레이션은 `정책 평가`와 `정책 발전`을 번갈아 가며 실행하는 과정
> 정책 평가 : 현재의 정책에 대해 정책 평가를 하여 가치함수를 구함, 벨만 기대 방정식
> 정책 발전 : 그 가치함수를 통해 발전
	* 정책 평가 과정에서 참 가치함수에 수렴할 때까지 계산하지 않아도 정책 평가와 발전을 한번씩 번갈아 실행하여도 가치함수가 참 가치함수에 수렴할 때 까지 계산한다면, 최적 가치함수도 참 가치함수에 수렴(Gerneralized Policy Iteration, GPI)
	* GPI의 정책 `평가 과정`을 수행하는 것이 강화학습에서는 MCP 혹은 TDP
	* GPI의 탐욕 정책 발전은 주어진 가치함수에 대해 새로운 정책을 얻는 과정, TDP은 타임스텝마다 가치함수를 현재 상태에 대해서만 업데이트
		* -> GPI에서처럼 ::모든 상태::의 정책을 발전 시킬수 ::없음::
	* TD Control = TDP + Greed policy 
		* -> 모든 상태에 대한 정책을 알아내는 것 대신 현재 순간에 가장 큰 가치를 갖는 행동을 하는 것!
> 		GPI			->	시간차 제어
> 		정책 평가 		->	시간차 예측
> 		탐욕 정책 발전 ->	탐욕 정책
	* 시간차 제어에서의 탐욕 정책은 GPI와 달리 환경 모델 P를 알 수 없음. 하지만 가치함수를 사용하지 않고, 현재 상태의 큐 함수를 보고 판단하면 모델 없이 계산 가능
		* GPI 탐욕 정책 발전 : ${\pi}’(s) = argmax_{a {\in} A}[R^a_s + {\gamma}P^a_{ss}’V(s’)]$
		* 큐함수를 사용한 탐욕 정책 : ${\pi}(s) = argmax_{a{\in}A}Q(s,a)$
	* 시간차 제어의 수식 : $Q(S_t, A_t) <- Q(S_t, A_t) + {\alpha}(Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))$
	* 시간차 제어에서 $[S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}]$를 샘플로 이용하여 큐함수 업데이트
	* TDC에서 큐함수 업데이트 : 현재 상태($S_t$)에서 탐욕 정책에 따라 행동($A_t$)를 선택하여 다음 시점의 상태($S_{t+1}$)가 되고, 다음 시점에서 탐욕 정책에 따라 행동($A_{t+1}$)을 결정하여 큐 함수($Q(S_{t+1}, A_{t+1})$)를 계산하여 현재 상태의 큐 함수($Q(S_t, A_t)$)를 계산
	* 단순한 탐욕 정책은 local minimum(또는 잘못된 학습)으로 갈 가능성이 높기 때문에 강화학습의 중요한 문제인 탐험(Exploration)을 해결하기 위해 ${\epsilon}$-탐욕 정책을 사용
> 		[image:70FA9822-B411-459D-9F0C-DE35CF8F569F-398-0000004FA77FA667/image.png]
	* ${\epsilon}$-탐욕 정책은 높은 확률로 탐욕 정책을 따르지만, 낮은 확률로 탐욕 정책에 반하는 행동(탐험, exploration)을 하는 경우가 생김
	* 단점은 최적의 큐함수를 찾았더라도 일정 확률로 탐험을 하는 경우가 생김. 이를 해결하기 위해 학습을 진행함에 따라 ${\epsilon}$의 값을 줄이는 방법도 있지만, 살사와 큐러닝에서는 일정한 값을 사용함
	* `살사`는 GPI의 정책 평가를 큐함수를 이용한 시간차 예측으로, 탐욕 정책 발전을 ${\epsilon}$-탐욕 정책으로 변화시킨 강화학습 알고리즘

4. 살사 코드 설명
	* 현재 상태에서 ${\epsilon}$-탐욕 정책에 따라 행동을 선택
	* 선택한 행동으로 환경에서 한 타임스텝을 진행
	* 환경으로부터 보상과 다음 상태를 받음
	* 다음 상태에서 ${\epsilon}$-탐욕 정책에 따라 다음 행동을 선택
	* (s,a,r,s’,a’)을 통해 큐함수 업데이트
5. 살사 코드의 실행 및 결과

<인상깊은 점>
* 처음에 용어들이 하나씩 나오기 시작할 때에는 대략적인 개념들이 보여 쉽게 넘어간 것들이 많았는데, 현재까지 읽어 오면서 상당히 많은 개념들을 다루고 지나왔다는 것을 알게 되었다. 너무 드문드문 읽어 정확한 정의가 가물가물 한 내용들은 다시 한 곳에 잘 정리하여 모아 놓아야 할 것 같다.

<질문>
* 시간이 지나 생각이 나지 않는 정책 이터레이션과 가치 이터레이션의 차이
* 시간차 제어(TDC)와 GPI는 서로 다른 알고리즘?, 마치 정책 이터레이션과 가치 이터레이션의 관계같은건가? 아니면 GPI에 추가 알고리즘을 더한 것이 TDC인건가?
* 시간차 제어(TDC)에서 현재 시점의 큐함수를 계산하기 위해 다음 시점의 큐 함수를 그 시점($t+1$)의 상태와 행동을 통해 계산하는데, 다음 시점의 큐 함수는 다시 그 다음 시점($t+2$)를 계산해야 하지 않나?
* SARSA부터 `강화학습`이라고 부르는 이유는 Q함수를 업데이트 하는 과정이 ::학습::과정이 포함 되어 있기 때문인건가?
