<요약>
1. 강화학습 알고리즘의 흐름
	* 순차적 행동 결정 문제를 정의하고 MDP형태로 문제 정의, 벨만 기대 방정식과 벨만 최적 방정식을 각각 정책 이터레이션과 가치 이터레이션을 통해 `참`값을 계산
	* 위의 두 이터레이션은 후에 살사(SARSA)로 발전하고, off-policy방법으로 큐러닝(Q-Learning)으로 이어짐
2. 정책 이터레이션
	* MDP로 정의되는 문제에서 찾고자 하는 것은 가장 높은 보상을 얻게 하는 정책! 하지만 처음부터 알 수 없기 때문에 임의의 정책을 시작으로 점차 발전 시켜나가는 방향으로 찾음
	* 정책 이터레이션은 어떤 정책(\pi)이 있을 때, 이를 가치함수를 통해 정책 평가를 하고, 발전을 시켜 더 나은 정책(\pi’)을 계산하는 것을 반복하여 최적의 정책으로 수렴을 시키는 것
3. 정책 평가
	* 정책의 평가 지수는 `가치함수`
	* 모든 환경에 대해 먼 미래에 대한 것 까지 고려를 한다면 계산량이 기하급수적으로 늘어나기 때문에, Dynamic programming(DP)를 활용해 작은 문제들로 나누어 순차적으로 계산
	* 주변 상태의 가치함수와 한 타임스텝의 보상만을 고려하여 현재 상태의 다음 가치함수를 계산
	* 한 스텝에 대한 계산은 `참 가치함수`는 아니지만 여러번 ::반복::하여 계산한다면, `참값에 수렴
	* 그리드 월드에서 가치함수의 계산 방식은 어떤 방향(상,하,좌,우 중 하나)로 이동하였을 때의 주어진 v_k(s’), R_s^a를 감가율을 이용하여 더하고, 각 방향을 선택할 확률을 곱하여 모든 행동에 대한 합을 하면, 현재 상태(s)에서 k+1번째 iteration을 진행한 가치함수의 결과가 나옴
4. 정책 발전
	* 앞선 정책 평가는 정책 발전을 위한 결과
	* 이 책에선 탐욕 정책 발전(GreedY Policy Improvement)를 사용
		0. Greedy Policy Improvement
		1. 정책을 통해 행동을 결정하기 위해 Qfunction을 정의
		2. 각 행동에 대한 Q function결과를 계산
		3. ARGMAX - Q function의 결과가 가장 큰 방향으로 이동

<인상깊은 점>
* 아직까지 강화학습은 `학습(Learning)`알고리즘보다는 ::결정(Decision)::알고리즘에 가까운 것 같음
* 모든 함수들이 수렴한다고 이야기 하지만, Local minimum과 global minimum를 구분하지는 않음(난이도를 쉽게 하기 위해서 언급하지 않은 것 같기도 함)

<질문>
* 용어에 대한 설명이 적어 아직은 이해가 어려움(SARSA, off-policy, Q-learning)
* 최적의 정책이 global minimum인 것은 어떻게 알 수 있지?
* Iteration과 시간의 흐름에 대한 개념이 헷갈린다(수식 3.6과 그림 3.8)
