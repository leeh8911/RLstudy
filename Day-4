<요약>
1. RLCode 깃허브 저장소
	* Http://github.com/rlcode/reinforcement-learning-kr
2. 정책 이터레이션 코드 설명
	* Policy-iteration.py - 실질적인 강화학습 예제 코드
	* Environment.py - 그리드 월드에 대한 환경 코드
	* init : PolicyIteration객체에 대한 초기화, 모든 환경에 대한 가치를 0으로 초기화 하고, 모든 환경에 대한 정책(각 방향으로 이동할 확률)을 균등하게 초기화, 도착 지점에 도달 하면 이동하지 못하게 설정하고, 감가율을 설정
	* Policy_evaluation : 다음 가치함수를 초기화 하고, 모든 상태에 대해서 가치 함수를 계산(도착 지점에 대한 가치함수는 0)하고 가치함수 table을 업데이트
	* Policy_improvement : 모든 상태 각각을 어떤 행동을 취해야지[보상 + 감가율 * 다음 상태의 가치함수]를 계산 하였을 때 최대가 되는지 비교하고, 최대인 행동(2개 이상일 수도 있음)인 확률을 계산
	* Get_action : 상태값을 입력으로 받고 0~1 사이의 무작위 수를 추출하고, 임의의 행동을 결정(정책의 확률을 누적하였을 때, 무작위 수가 그 확률을 넘어서는 행동 index를 반환)
	* Get_policy : 상태값을 입력으로 받아 해당 상태의 정책 table을 반환
	* Get_value : 상태값을 입력으로 받아 해당 상태의 가치함수를 반환
	
4. 정책 이터레이션 코드 실행
	* 실제 강화학습은 실행시키면 정책 평가와 정책 발전을 자동으로 반복하지만, 정책 이터레이션 코드는 실습을 하기 위해 각각의 계산을 수동으로 실행함

<인상깊은 점>
* 지금까지는 최적의 정책을 찾는 방법은 학습 알고리즘보다는 maneuver decision에 가까운 것 같다. 추후에 공부하게 될 DQN(?)에 대한 내용은 어떻게 구성될 지 궁금
* 예시가 그리드 월드이다보니, 현재 진행하고 있는 프로젝트에서 occupied영역을 찾아가는 과정을 유사한 방법으로 적용할 수 있지 않을까 하는 생각도 듦

<질문>
* 현재의 정책이 수렴된 결과인지 확인 할 수 있는 방법?
* 그림 3.15에서 evaluation - improvement를 한번씩 실행한 결과에서 [1,0], [0,1] 상태의 정책이 저렇게 나온 이유는?(직접 계산을 해볼 필요가 있을 듯)
