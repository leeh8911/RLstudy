<요약>

1. 살사의 한계
    - 살사에서의 큐함수 업데이트 수식

    $$Q(s,a) \leftarrow Q(s,a) + {\alpha}(r+{\gamma}Q(s^{\prime}, a^{\prime}) - Q(s,a))$$

    - 탐험을 위해 선택한 \epsilon-탐욕 정책 때문에 에이전트는 오히려 최적 정책을 학습하지 못하고 잘못된 정책을 학습할 수 있음
    - 살사는 온폴로시 시간차 제어(On-policy Temporal-Difference Control), 즉 자신이 행동하는 대로 학습하는 시간차 제어이기 때문에 탐험을 위해 선택한 \epsilon-탐욕 정책 때문에 오히려 최적 정책을 학습하지 못하고 잘못된 정책을 학습함
    - 강화학습에서 탐험은 절대적으로 필요하지만 잘못된 탐험이 잘못된 정책을 학습할 수 있음, 이를 해결하기 위한 것이 오프폴로시 시간차 제어(Off-Policy Temporal-Difference Control, Q-Learning)
2. 큐러닝 이론
    - 큐러닝은 1989년 Chris Watkins에 의해 소개
    - 오프폴로시는 현재의 행동하는 정책과 독립적으로 학습함, 즉 행동하는 정책과 학습하는 정책을 분리
    - 살사는 s’상태에서 \epsilon-탐욕 정책에 따라 다음 행동을 선택한 후에 그것을 학습샘플로 사용
    - 큐러닝에서는 s’을 일단 알게되면 그 생태에서 가장 큰 큐함수를 현재 큐함수의 업데이트에 사용
    - 큐러닝에서 현재 상태의 큐함수를 업데이트 하기 위해 필요한 샘플은 <s, a, r, s’>
    - 큐러닝과 벨만 최적 방정식

        $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + {\alpha}(R_{t+1} + {\gamma}max_{a^{\prime}}Q(S_{t+1}, a^{\prime})-Q(S_t, A_t))$$

        $$q_*(s,a) = {\mathbb E}[R_{t+1} + {\gamma}max_{a^{\prime}}q_*(S_{t+1},a^{\prime}) |S_t = s, A_t = a]$$

    - 큐러닝에서 보상 R_{t+1}은 실제 에이전트가 환경에서 받는 값이므로 기대값 E를 빼면 두 식은 동일함
    - 큐함수를 업데이트 하기 위해서 살사는 벨만 기대 방정식을 사용, 큐러닝은 벨만 최적 방정식을 사용

        > 벨만 **기대** 방정식 → 정책 이터레이션 → 살사

        > 벨만 최적 방정식 → 가치 이터레이션 → 큐러닝

    - 큐러닝을 통해 학습하면 에이전트가 영 좋지 못한 행동을 하더라도 현재 상태 s의 큐함수를 업데이트 할 때 포함되지 않음
    - 큐러닝은 살사에서의 딜레마였던 탐험 - 최적 정책 학습의 문제를 정책을 분리시키고 행동 선택은 \epsilon-탐욕정책으로, 업데이트는 벨만 최적 방정식을 이용함으로써 해결
3. 큐러닝 코드 설명
4. 큐러닝 코드의 실행 결과

<인상깊은 점>

- 잘못된 탐험으로 인해 일종의 local minima에 수렴해 버리는 문제를 생각보다 간단한 방식으로 해결했다는 점
- 연속적으로 스터디하고 꾸준히 반복하지 않으면 지속적으로 연결되는 내용에 대한 이해도가 떨어져서 따라가기 어려운것 같다.

<질문>

- <empty>
