<요약>
* 다이나믹 프로그래밍은 큰 문제를 작은 문제들로 나누어 해결하는 방법
* 다이나믹 프로그래밍으로 벨만 기대 방정식을 푸는 것 -> 정책 이터레이션
* 다이나믹 프로그래밍으로 벨만 최적 방정식을 푸는 것 -> 가치 이터레이션
* 순차적 행동 결정 문제를 해결하는 방법 중 하나가 강화학습
	1. 순차적 행동 문제를 MDP로 전환
	2. 가치함수를 벨만 방정식으로 반복적으로 계산
	3. 최적 가치함수와 최적 정책을 찾는다
* 벨만 방정식을 푼다는 것은 최적의 가치함수(v*)를 찾는 것
> v* = max_{a} E[R_{t+1} + \gv*(S_{t+1}) | S_{t} = s, A_{t} = a]
> 벨만 최적 방정식
* 다이나믹 프로그래밍은 커다랗고 복잡한 문제를 작고 간단한 형태로 나누어 순차적으로 해결하는 것
* 순차적 행동 문제를 푸는 다이나믹 프로그래밍은 `정책 이터레이션`과 `가치 이터레이션`

<인상깊은 점>
* 아직 벨만 방정식이 어떻게 활용되는지 감은 잡히지는 않지만, 다이나믹 프로그래밍에 대해서는 복잡한 문제를 나누어 해결한다는 아이디어가 여러 알고리즘의 변천사와 닮은 것 같다.

<질문>
* 벨만 기대 방정식을 다이나믹 프로그래밍으로 푼다는 것은 가치 함수의 `참`값을 구하고자 하는건가? 아미녀 `최적 가치 함수`를 찾는 건가? 그림 3.2를 보면 세로 방향으로의 processing은 양 옆에 있는 (같은 iteration안에 있는) 블럭과의 연결에 의해 얽혀있는데 최적 가치 함수를 구하는데에 어떻게 사용되는건가?
-> 즉 v(s_2)의 참값을 구하기 위해 iteration을 한다면 v(s_1)의 결과가 반영되기 때문에 문제가 복잡해질 것 같다.
