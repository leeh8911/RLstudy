<요약>
	* 강화학습과 다이나믹 프로그래밍의 차이는 강화학습은 환경의 모델을 몰라도 환경과의 상호작용을 통해 최적의 정책을 학습
	* 예측 - 몬테카를로 예측, 시간차 예측/ 제어 - 시간차 제어(SARSA, State -> Action -> Reward -> State’ -> Action), 오프폴로시 제어(큐러닝)
1. 사람의 학습 방법과 강화학습의 학습 방법
	* 강화학습은 환경의 모델 없이 환경이라는 시스템의 입력과 출력 사이의 관계를 학습
	* DP : 차원(상태의 수)가 증가 할 수록 계산 복잡도가 증가
		-> 환경에 대한 정확한 지식을 가지고 모든 상태에 대해 동시에 계산
	* 사람 : 모든 상태에 대한 경우의 수를 계산하기보단 경험하고 복기를 하면서 잘못된 점을 학습함
	* 강화학습 : 일단 해봄 -> 평가 -> 발전 (반복) 을 통해 학습, 즉 사람의 학습방법과 유사한 전략으로 학습
2. 강화학습의 예측과 제어
	* DP의 정책 이터레이션은 정책 평가와 정책 발전으로 이루어져 있고, 정책 평가에는 벨만 기대 방정식이 사용됨
		$v_{\pi} = E_{\pi}[R_{t+1} + {\gamma}v_{\pi}(S_{t+1})|S_{t} = s]$
	* MDP로 정의되는 문제를 풀 때 ::벨만 기대방정식의 기대값 $E_{\pi}$를 어떻게 계산하는가::가 중요
	* DP와 달리 강화학습은 적당한 추론을 통해 원래 참 가치함수의 값을 `예측`
	* DP에서 정책 평가와 정책 발전을 합쳐서 `정책 이터레이션`이라 하고, 강화학습에선 예측과 함께 정책을 발전 시키는 것을 `제어`라고 함
3. 몬테카를로 근사의 예시
	* Monte Carlo 근사는 ::무작위로 무엇인가를 해본다::는 의미로 무작위로 무엇인가를 무수히 많이 하면 근사치는 참값에 가까워 짐(대수의 법칙?)
4. 샘플링과 몬테카를로 예측
	* 가치함수를 추정할 때, 에이전트가 한 번 환경에서 에피소드를 진행하는 것이 샘플링.
	* 샘플링을 통해 무수히 많은 샘플을 추출한 다음, 평균을 계산 한 것을 참 가치함수의 값이라 추정
	* 샘플링을 통해 정책에 따른 가치함수를 구하려면 `현재 정책에 따라서 계속 행동`
	* 끝이 있는 반환값의 정의
		-> $G_{t} = R_{t+1} + {\gamma}R_{t+2} + ... + {\gamma}^{T-t+1}R_{T}$
	* 시작 상태 S로부터 마침 상태 T까지 진행하는 것을 에피소드라 정의
	* 에피소드에서 각 시점마다의 반환값을 얻어도 참 가치함수를 계산하기는 어려움
	* 벨만 기대 방정식을 계산하기 위해서는 환경 모델 P, R을 알아야 하지만, Monte Calro 에측에서는 여러 에피소드를 통해 구한 반환값의 평균을 통해 v_{\pi}(s)를 추정
		-> $v_{\pi} ~ {\frag}{1}{N(s)}{\SIGMA}^{N(s)}_{i=1}{G_i(s)}$
	* 위의 수식은 `여러번의 에피소드`에서 `s라는 상태`를 방문해서 얻은 `반환값들의 평균`을 통해 `참 가치함수`를 추정하는 식
	* 현재 정책에 따라 무수히 많은 에피소드를 진행하면 충분한 반환값을 모을 수 있고, 반환값들의 평균을 통해 상당히 정확한 가치함수를 얻을 수 있음
		$ V_{n+1} = {\frac}{1}{n}{\SIGMA}^{n}_{i=1}G_{i} $
		$                = {\frac}{1}{n}(G_{n} + {\SIGMA}^{n-1}_{i=1}G_{i}) $
		$                = {\frac}{1}{n}(G_{n}+(n-1){\frac}{1}{n-1}{\SIGMA}^{n-1}_{i=1}G_{i}) $
		$ 		  = {\frac}{1}{n}(G_n+(n-1)V_n) $
		$                = {\frac}{1}{n}(G_n + nV_n - V_n) $
		$                = V_n + {\frac}{1}{n}(G_n - V_n) $
	* 특정 상태에서의 가치함수 업데이트는 샘플링을 통해 에이전트가 방문 할 때마다 업데이트
	* $G(s) - V(s)$ : 오차
	* $1/n$            : 스텝사이즈, 일반적으로 ${\alpha}$로 표현하는 경우가 많음
	* $ V(s) <- V(s) + {\alpha}(G(s) - V(s))$ 몬테카를로 예측을 통한 가치함수 업데이트의 일반식

<인상깊은 점>
* 몬테카를로 예측, 근사에 대해서는 다양한 분야에서 사용되는 강력한 근사 방법인 것 같다. 아직까지는 몬테카를로 예측을 통한 강화학습 방법을 어떻게 구현할 수 있을지 감이 잡히지는 않지만, top-down으로 내려가면서 차근차근 이해하는 과정이라 생각이 된다.

<질문>
* 벨만 기대 방정식과 여러 에피소드의 평균 반환값 사이의 관계가 어떻게 유사할 수 있는지?
* 학습에 대한 부분이 몬테카를로 근사에선 어떻게 구현되는지?
